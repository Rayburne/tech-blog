<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[Youmi Tech Blog.]]></title><description><![CDATA[I dream of painting and then I paint my dream.]]></description><link>http://tech.youmi.net</link><generator>RSS for Node</generator><lastBuildDate>Mon, 25 Apr 2016 12:21:24 GMT</lastBuildDate><atom:link href="http://tech.youmi.net/rss.xml" rel="self" type="application/rss+xml"/><pubDate>Mon, 25 Apr 2016 12:21:24 GMT</pubDate><item><title><![CDATA[No title]]></title><description><![CDATA[<p>在分布式系统中，各个机器都有程序运行的本地日志，有时为了分析需求，不得不这些分散的日志汇总需求，相信很多人会选择 Rsync，Scp 之类，但它们的实时性不强，而且也会带来名字冲突的问题。扩展性差强人意，一点也不优雅。</p>
<p>现实中，我们就碰到了这样的需求：实时汇总线上多台服务器的 Nginx 日志。Flume 立功了。</p>
<h1 id="flume-">Flume 简介</h1>
<p><a href="https://flume.apache.org/"><strong>F</strong>lume</a> 是一个分布式，可靠高效的日志收集系统，它允许用户自定义数据传输模型，因此可扩展性也强。也有较强的容错和恢复机制.
以下是几个重要的概念</p>
<ul>
<li>Event：Event 是 Flume 数据传输的基本单元。flume 以事件的形式将数据从源头传送到最终的目的。</li>
<li>Agent：Agent包含 Sources, Channels, Sinks 和其他组件，它利用这些组件将events从一个节点传输到另一个节点
或最终目的。</li>
<li>Source：Source负责接收events，并将events批量的放到一个或多个Channels。</li>
<li>Channel：Channel位于 Source 和 Sink 之间，用于缓存进来的events，当Sink成功的将events发送到下一跳的channel或最终目的，events从Channel移除。</li>
<li>Sink：Sink 负责将 events 传输到下一跳或最终目的，成功完成后将events从channel移除。</li>
</ul>
<p><img src="http://zheng-ji.info/images/2016/04/flume.jpg" alt="flume"></p>
<ul>
<li>Source 就有 Syslog Source, Kafka Source,HTTP Source, Exec Source Avro Source 等。</li>
<li>Sink 有 Kafka Sink, Avro Sink, File Roll Sink, HDFS Sink 等。</li>
<li>Channel 有 Memory Channel,File Channel 等</li>
</ul>
<p>它提供了一个骨架，以及多种 Source, Sink, Channel, 让你设计合适的数据模型。事实上也可以多个 Flume 联动完成，就像地铁的车厢一样。</p>
<h1 id="-">定义数据流模型</h1>
<p>回到我们开头的场景,我们要将多台服务器的 Nginx 日志进行汇总分析，</p>
<p>分成两个 flume 来实现</p>
<ul>
<li>Flume1 数据流是 Exec Source -&gt; Memory Channel -&gt; Avro Sink,部署在业务机器上</li>
<li>Flume2 数据流是 Avro Source -&gt; Memory Channel -&gt; FileRoll Sink,部署在目标机器上</li>
</ul>
<p><img src="http://zheng-ji.info/images/2016/04/flume1toflume2.jpg" alt="flume"></p>
<h1 id="-">需要的准备</h1>
<p>你需要安装</p>
<ul>
<li>下载 <a href="https://flume.apache.org/download.html">Flume</a></li>
<li>安装 JavaSDk,并在下载解压之后的 conf/flume-env.sh，配置</li>
</ul>
<pre><code class="lang-sh"><span class="hljs-meta"># 我用的是oracle-java-8</span>
export JAVA_HOME=<span class="hljs-meta-keyword">/usr/</span>lib<span class="hljs-meta-keyword">/jvm/</span>java<span class="hljs-number">-8</span>-oracle<span class="hljs-meta-keyword">/jre/</span>
</code></pre>
<ul>
<li>思考你的数据流动模型，编写配置，如上文所说的Flume1, tail2avro.conf  ：</li>
</ul>
<pre><code>agent<span class="hljs-selector-class">.sources</span> = s1
agent<span class="hljs-selector-class">.channels</span> = c1
agent<span class="hljs-selector-class">.sinks</span> = k1

agent<span class="hljs-selector-class">.sources</span><span class="hljs-selector-class">.s1</span><span class="hljs-selector-class">.type</span>=exec
agent<span class="hljs-selector-class">.sources</span><span class="hljs-selector-class">.s1</span><span class="hljs-selector-class">.command</span>=tail -F &lt;Your File Path&gt;
agent<span class="hljs-selector-class">.sources</span><span class="hljs-selector-class">.s1</span><span class="hljs-selector-class">.channels</span>=c1

agent<span class="hljs-selector-class">.channels</span><span class="hljs-selector-class">.c1</span><span class="hljs-selector-class">.type</span>=memory
agent<span class="hljs-selector-class">.channels</span><span class="hljs-selector-class">.c1</span><span class="hljs-selector-class">.capacity</span>=<span class="hljs-number">10000</span>
agent<span class="hljs-selector-class">.channels</span><span class="hljs-selector-class">.c1</span><span class="hljs-selector-class">.transactionCapacity</span>=<span class="hljs-number">10000</span>

agent<span class="hljs-selector-class">.sinks</span><span class="hljs-selector-class">.k1</span><span class="hljs-selector-class">.type</span> = avro
agent<span class="hljs-selector-class">.sinks</span><span class="hljs-selector-class">.k1</span><span class="hljs-selector-class">.hostname</span> = &lt;Your Target Address&gt;
agent<span class="hljs-selector-class">.sinks</span><span class="hljs-selector-class">.k1</span><span class="hljs-selector-class">.port</span> = &lt;Your Target Port&gt;
agent<span class="hljs-selector-class">.sinks</span><span class="hljs-selector-class">.k1</span><span class="hljs-selector-class">.channel</span>=c1
</code></pre><p>Flume2 中的 avro2file.conf</p>
<pre><code><span class="hljs-built_in">agent</span>.sources = s1
<span class="hljs-built_in">agent</span>.channels = c1
<span class="hljs-built_in">agent</span>.sinks = k1

<span class="hljs-built_in">agent</span>.sources.s1.<span class="hljs-built_in">type</span> = avro
<span class="hljs-built_in">agent</span>.sources.s1.bind = &lt;Your Address&gt;
<span class="hljs-built_in">agent</span>.sources.s1.port = &lt;Your Port&gt;
<span class="hljs-built_in">agent</span>.sources.s1.channels = c1

<span class="hljs-built_in">agent</span>.sinks.k1.<span class="hljs-built_in">type</span> = file_roll
<span class="hljs-built_in">agent</span>.sinks.k1.sink.directory = /data/<span class="hljs-built_in">log</span>/ngxlog
<span class="hljs-meta"># 滚动间隔</span>
<span class="hljs-built_in">agent</span>.sinks.k1.sink.rollInterval = <span class="hljs-number">86400</span>
<span class="hljs-built_in">agent</span>.sinks.k1.channel = c1

<span class="hljs-built_in">agent</span>.channels.c1.<span class="hljs-built_in">type</span> = memory
<span class="hljs-meta"># 队列里 Event 的容量</span>
<span class="hljs-built_in">agent</span>.channels.c1.capacity = <span class="hljs-number">10000</span>
<span class="hljs-built_in">agent</span>.channels.c1.transactionCapacity = <span class="hljs-number">10000</span>
<span class="hljs-built_in">agent</span>.channels.c1.keep-<span class="hljs-built_in">alive</span> = <span class="hljs-number">60</span>
</code></pre><ul>
<li>启动运行</li>
</ul>
<pre><code># 启动flume1
bin/flume-ng agent -<span class="hljs-keyword">n</span> agent -c <span class="hljs-keyword">conf</span> -f <span class="hljs-keyword">conf</span>/tail2avro.<span class="hljs-keyword">conf</span> \
-Dflume.root.logger=WARN

# 启动flume2
bin/flume-ng agent -<span class="hljs-keyword">n</span> agent -c <span class="hljs-keyword">conf</span> -f <span class="hljs-keyword">conf</span>/avro2file.<span class="hljs-keyword">conf</span> \
-Dflume.root.logger=INFO
</code></pre><h2 id="-">参考</h2>
<ul>
<li><a href="https://flume.apache.org/FlumeUserGuide.html">FlumeUserGuide</a> 官方的 FlumeUserGuide</li>
</ul>
]]></description><link>http://tech.youmi.net/2016/04/150658153.html</link><guid isPermaLink="true">http://tech.youmi.net/2016/04/150658153.html</guid><dc:creator><![CDATA[zheng-ji]]></dc:creator><pubDate>Mon, 25 Apr 2016 12:18:29 GMT</pubDate></item><item><title><![CDATA[No title]]></title><description><![CDATA[<p>Ansible 在使用的过程中，如果机器数量比较固定，且变更不多的情况下，可在 <code>/etc/ansible/hosts</code> 文件里面配置固定的组合机器IP，并给他起组的别名，执行 <code>ansible</code> 脚本便可以通过别名找到相应的机器。</p>
<pre><code><span class="hljs-string">[webservers]</span>
<span class="hljs-number">111.222.333.444</span> ansible_ssh_port=<span class="hljs-number">888</span>
</code></pre><p>假如你有很多台机器，且机器经常变更导致IP时常变换，你还想把IP逐个写入 <code>/etc/ansible/hosts</code> 就不现实了。你也许会问，若不把IP写进 <code>/etc/ansible/hosts</code>，那不是没法用 <code>ansible</code> 指挥这些机器？
感谢 <code>Ansible Dynamic Inventory</code>， 如果我们能通过编程等手段获取变更机器的IP，我们还是有办法实现的。</p>
<hr>
<h3 id="dynamic-inventory-">Dynamic Inventory 的原理</h3>
<ul>
<li>通过编程的方式,也就是动态获取机器的 json 信息;</li>
<li>Ansible 通过解析这串 json 字符串;</li>
</ul>
<pre><code>ansible -<span class="hljs-selector-tag">i</span> yourprogram<span class="hljs-selector-class">.py</span> -m raw  -<span class="hljs-selector-tag">a</span> <span class="hljs-string">'cd /home'</span>
</code></pre><p>Ansible dDynamic Inventory 对程序返回的 json 的转义是这样的：</p>
<pre><code>{<span class="hljs-attr">"devtest-asg"</span>: {<span class="hljs-attr">"hosts"</span>: [<span class="hljs-string">"172.31.21.164"</span>], <span class="hljs-attr">"vars"</span>: {<span class="hljs-attr">"ansible_ssh_port"</span>: <span class="hljs-number">12306</span>}}}
</code></pre><p>翻译一下就是  <code>/etc/ansible/hosts</code> 中的:</p>
<pre><code><span class="hljs-string">[devtest-asg]</span>
<span class="hljs-number">172.31.21.164</span> ansible_ssh_port=<span class="hljs-number">12306</span>
</code></pre><hr>
<h3 id="-">一个实战的例子</h3>
<p>官方文档对 Inventory 仅作概念性描述，阅读完后仍是一头雾水，不知如何下手。
让我们用一个例子来豁然开朗吧。
我们使用 AWS 的 AutoScaling Group，以下简称 ASG，ASG 会在某种自定义的条件下会自动开启和关闭机器，这给我们在辨别IP，定位机器的时候造成困扰。因此我们需要　<code>Ansible Dynamic Inventory</code></p>
<p>我们使用 AWS 的｀boto｀库来获取ASG的实例信息.以下程序(get_host.py)中要实现的方法就是列出返回机器信息的 json 串。</p>
<pre><code class="lang-python"><span class="hljs-comment">#!/usr/bin/env python</span>
<span class="hljs-comment"># -*- coding: utf-8 -*-</span>
<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> boto
<span class="hljs-keyword">import</span> boto.ec2
<span class="hljs-keyword">import</span> boto.ec2.autoscale

<span class="hljs-type">AWS_REGION</span> = '<span class="hljs-type">BBB</span>'
<span class="hljs-type">AWS_ACCESS_KEY</span> = 'xxxx'
<span class="hljs-type">AWS_SECRET_KEY</span> = 'yyy'

<span class="hljs-literal">result</span> = {}
def getData():
    conn_as = boto.ec2.autoscale.connect_to_region(
            'cn-north-<span class="hljs-number">1</span>',
            aws_access_key_id=<span class="hljs-type">AWS_ACCESS_KEY</span>,
            aws_secret_access_key=<span class="hljs-type">AWS_SECRET_KEY</span>)
    group = conn_as.get_all_groups(names=['devtest-asg'])[<span class="hljs-number">0</span>]
    conn_ec2 = boto.ec2.connect_to_region(
            <span class="hljs-type">AWS_REGION</span>,
            aws_access_key_id=<span class="hljs-type">AWS_ACCESS_KEY</span>,
            aws_secret_access_key=<span class="hljs-type">AWS_SECRET_KEY</span>)

    instance_ids = [i.instance_id <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> group.instances]
    reservations = conn_ec2.get_all_instances(instance_ids)
    instances = [i <span class="hljs-keyword">for</span> r <span class="hljs-keyword">in</span> reservations <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> r.instances]

    <span class="hljs-literal">result</span>['devtest-asg'] = {}
    <span class="hljs-literal">result</span>['devtest-asg']['hosts'] = []
    <span class="hljs-keyword">for</span> r <span class="hljs-keyword">in</span> reservations:
        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> r.instances:
            <span class="hljs-literal">result</span>['devtest-asg']['hosts'].append('%s' % i.private_ip_address)
            <span class="hljs-literal">result</span>['devtest-asg']['vars'] = {'ansible_ssh_port': <span class="hljs-number">36000</span>}

def getlists():
    getData()
    print json.dumps(<span class="hljs-literal">result</span>)

getlists()
</code></pre>
<p>执行以下命令就可以愉快地使用 Ansible 了，其中 <code>devtest-asg</code> 是ASG的别名：</p>
<pre><code>ansible -<span class="hljs-selector-tag">i</span> get_host<span class="hljs-selector-class">.py</span>  devtest-asg -m raw -<span class="hljs-selector-tag">a</span> <span class="hljs-string">'ls /'</span>
</code></pre>]]></description><link>http://tech.youmi.net/2016/04/145599925.html</link><guid isPermaLink="true">http://tech.youmi.net/2016/04/145599925.html</guid><dc:creator><![CDATA[zheng-ji]]></dc:creator><pubDate>Wed, 06 Apr 2016 14:26:28 GMT</pubDate></item><item><title><![CDATA[No title]]></title><description><![CDATA[<p>ELK是指elastic提供的一整套数据收集，存储，搜索，及展示方案。 由于部署及扩容方便，非常适合小团队快速搭建数据分析平台。ELK分别代指 Elasticsearch，Logstash，Kibana三个产品。</p>
<ul>
<li><a href="https://www.elastic.co/products/elasticsearch"><strong>E</strong>lasticsearch</a>: 分布式，实时，全文搜索引擎，最核心的部分，也是接下来主要介绍的内容</li>
<li><a href="https://www.elastic.co/products/logstash"><strong>L</strong>ogstash</a>: 非常灵活的日志收集工具，不局限于向 Elasticsearch 导入数据，可以定制多种输入，输出，及过滤转换规则</li>
<li><a href="https://www.elastic.co/products/kibana"><strong>K</strong>ibana</a>: 提供对于 Elasticsearch 数据的搜索及可视化功能。并且支持开发人员自己按需开发插件。</li>
</ul>
<h1 id="elk-">ELK在广告系统监控中的应用</h1>
<p>广告系统对于请求的响应时间非常敏感，此外，对于并发请求数要求也很高。因此，我们需要从请求响应时间，以及QPS两个指标，来检测系统的性能; 同时，为了定位瓶颈，我们需要把这两个指标，分拆到请求处理过程中的每个组件去看。</p>
<p>此外，为了优化全球用户访问的速度，我们在全球部署了多个节点。因此，整个系统不是在一个内网环境，对监控数据的实时收集提出了考验。</p>
<p>因此，我们需要一套灵活的监控统计库，能够非侵入的注册到业务代码中; 并且在一个统一的入口，实时监控每个节点的运行情况。</p>
<p><img src="https://cloud.githubusercontent.com/assets/839287/13384488/f2339846-ded0-11e5-8b62-368e9d4de3de.png" alt="elk"></p>
<p>我们采用方案是:</p>
<ul>
<li>每个节点，有一个 Collector，负责收集监控数据，按照节点/服务/时间维度做聚合，实时写入 Kinesis 队列</li>
<li>Logstash 将 Kinesis 中的监控数据，实时导入到 Elasticsearch</li>
<li>Kibana 后台上，通过定制查询图表，可以统计对比每个节点/服务/组件的监控数据</li>
<li>我们采用默认的 Logstash 日志导入策略，按天索引，使用<a href="https://www.elastic.co/guide/en/elasticsearch/client/curator/current/index.html">curator</a>工具，定期对历史数据删除/优化。</li>
<li>此外，通过 <a href="https://www.elastic.co/products/watcher">Watcher</a> 插件，定制告警信息，从而在系统出现问题时及时处理。</li>
</ul>
<p>这套机制，很好地满足了我们对于系统监控的需求，帮助我们分析，定位优化系统瓶颈。</p>
<h1 id="elasticsearch-">Elasticsearch简介</h1>
<p>Elasticsearch 是一个分布式，实时，全文搜索引擎。所有操作都是通过 RESTful 接口实现; 其底层实现是基于 Lucene 全文搜索引擎。数据以JSON文档的格式存储索引，不需要预先规定范式。</p>
<p>和传统数据库的术语对比一下，也许能够帮助我们对 Elasticsearch 有一个更加感性的认识</p>
<table>
<thead>
<tr>
<th>RDS</th>
<th>Elasticsearch</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>database</td>
<td>index</td>
<td></td>
</tr>
<tr>
<td>table</td>
<td>type</td>
<td></td>
</tr>
<tr>
<td>primary key</td>
<td>id</td>
<td></td>
</tr>
<tr>
<td>row</td>
<td>JSON document</td>
<td>文档是最基本的数据存储单元，因此也可以称 Elasticsearch 为文档型 NoSQL 数据库</td>
</tr>
<tr>
<td>column</td>
<td>field</td>
<td></td>
</tr>
<tr>
<td>schema</td>
<td>mapping</td>
<td>可以支持动态范式，也可以将范式指定下来，从而优化对数据索引和查询</td>
</tr>
<tr>
<td>index</td>
<td>(all)</td>
<td>所有的字段都是被索引的，因此不需要手动指定索引</td>
</tr>
<tr>
<td>SQL</td>
<td>query DSL</td>
<td>不同于我们习惯的SQL语句，需要构造繁冗的JSON参数来实现复杂的数据统计查询功能</td>
</tr>
</tbody>
</table>
<h2 id="-">使用简介</h2>
<p>所有操作都是通过RESTfull接口完成。请求URI指定了文档的&quot;路径&quot;。
注意到语义通过请求的HTTP方法不同区分开来:</p>
<ul>
<li>创建 <code>POST /{index}/{type} {&quot;field&quot;: &quot;value&quot;, ...}</code></li>
<li>创建/更新 <code>PUT /{index}/{type}/{id} {&quot;field&quot;: &quot;value&quot;, ...}</code></li>
<li>判断一个文档是否存在 <code>HEAD /{index}/{type}/{id}</code></li>
<li>获取一个文档 <code>GET /{index}/{type}/{id}</code></li>
<li>删除 <code>DELETE /{index}/{type}/{id}</code></li>
</ul>
<p>说明一下<code>POST</code>和<code>PUT</code>的<a href="http://stackoverflow.com/questions/630453/put-vs-post-in-rest">区别</a>: <code>POST</code> 永远是创建新的。<code>PUT</code> 可以表示创建，但是如果指定的URI存在，则含义为更新。换句话说，一个<code>PUT</code>请求，重复执行，结果应该是一样的。因此，在 Elasticsearch 的API，<code>POST</code> 表示创建新的文档，并由 Elasticsearch 自动生成<code>id</code>; 而 <code>PUT</code> 方法需要指定文档<code>id</code>，含义为&quot;创建，若存在则更新&quot;。</p>
<p>另外，需要提一下版本的概念。因为 Elasticsearch 中的文档是不可变的(immutable)，所以每个文档会有一个 版本号(version)字段，每次更新，实际上是将旧的版本标记未删除，并创建了一个新版本(版本号+1)。这个开销是很大的。因此，在实际使用中，需要尽量避免频繁的更新操作。</p>
<p>为了满足一些复杂的数据统计，仅仅上述的增删改查是不够的，为了充分使用 Elasticsearch的搜索功能，还需要学习 query DSL 的使用。query DSL 写起来比较复杂，这里仅仅列举一下和SQL关键字的对应。具体使用还得参考文档。</p>
<pre><code>SQL            | query DSL
-------        | --------
=              | {<span class="hljs-string">"term"</span>: {field: val}
IN             | {<span class="hljs-string">"terms"</span>: {field: [val, ...]}
LIKE           | {<span class="hljs-string">"wildcard:"</span> {field: pattern}}
BETWEEN AND    | {<span class="hljs-string">"range"</span>: {field: {<span class="hljs-string">"gt"</span>: val, <span class="hljs-string">"lt"</span>: val}}}
AND / OR / NOT | {<span class="hljs-string">"bool"</span>: {<span class="hljs-string">"must"</span>/<span class="hljs-string">"should"</span>/<span class="hljs-string">"must_not"</span>: ...}
Aggregations   | {<span class="hljs-string">"aggs"</span>: ...}
JOIN           | {<span class="hljs-string">"nestted"</span>/<span class="hljs-string">"has_child"</span>/<span class="hljs-string">"has_parent"</span>: ...}
</code></pre><p>随便列两个查询语句，感受一下:</p>
<p><code>SELECT * FROM megacorp.employee WHERE age &gt; 30 AND last_name = &quot;smith&quot;</code></p>
<pre><code>GET /megacorp/employee/_search
{
  <span class="hljs-string">"query"</span>: {
    <span class="hljs-string">"filtered"</span>: {
      <span class="hljs-string">"filter"</span>: {
        <span class="hljs-string">"range"</span>: { <span class="hljs-string">"age"</span>: { <span class="hljs-string">"gt"</span>: <span class="hljs-number">30</span> } }
      },
      <span class="hljs-string">"query"</span>: {
        <span class="hljs-string">"match"</span>: {
          <span class="hljs-string">"last_name"</span>: <span class="hljs-string">"smith"</span>
        }
      }
    }
  }
}
</code></pre><p><code>SELECT interests，avg(age) FROM megacorp.employee GROUP BY interests</code></p>
<pre><code>GET /megacorp/employee/_search
{
  <span class="hljs-string">"aggs"</span>: {
    <span class="hljs-string">"all_interests"</span>: {
      <span class="hljs-string">"terms"</span>: { <span class="hljs-string">"field"</span>: <span class="hljs-string">"interests"</span> },
      <span class="hljs-string">"aggs"</span>: {
        <span class="hljs-string">"avg_age"</span>: {
          <span class="hljs-string">"avg"</span>: {
            <span class="hljs-string">"field"</span>: <span class="hljs-string">"age"</span>
          }
        }
      }
    }
  }
}
</code></pre><p>从个人的使用经验来说，熟练掌握 query DSL 的难度还是不小的 (毕竟大家都习惯的SQL的简洁直接)。此外，由于输入输出都是嵌套很深的JSON，解析起来也比较麻烦。为了降低使用门槛，一般都会有从SQL翻译的组件。比如Hive之于Hadoop，SparkSQL 之于 Spark。<a href="http://github.com/NLPchina/elasticsearch-sql">elasticsearch-sql</a>这个项目，提供了类似了SQL翻译功能。</p>
<p>当然，Elasticsearch 最突出的地方在于对于全文搜索的支持。全文搜索的原理，以及在 Elasticsearch中具体如何全文搜索，这里略去不表。下面介绍下 Elasticsearch 集群的实现。</p>
<h2 id="elasticsearch-">Elasticsearch 集群</h2>
<p>Elasticsearch 是分布式的架构，可以通过新增节点的方式水平扩容，并提供一定的容错性。那么，它是怎么做到的呢? 我们一个一个概念，细细道来:</p>
<ul>
<li><p><strong>节点(node)</strong>: 是一个独立的 Elasticsearch 运行实例。可以是一台机器上多个实例，当然更常见的选择是部署在不同的机器上。多个互通节点在一起组成了一个 <strong>集群 (cluster)</strong>。不同于常见的master/slave集群架构，Elasticsearch 集群中的节点是同构的，每个节点都可以处理请求，并将自己不能处理的请求&quot;重定向&quot;到目标节点。</p>
</li>
<li><p><strong>索引(index)</strong>: 一个索引，分成了多个分片。当将文档写入一个索引时，根据id做切割，交由某个具体的分片去完成写入操作。因此越多的分片数，提供了更好的并发写入性能。分片数目在索引创建就不能更改。</p>
</li>
<li><p><strong>分片(shard)</strong>: 即一个 Lucene 实例，从数据库的角度来理解，可以视为一个分区(partition)。</p>
</li>
<li><p><strong>镜像分片(replica)</strong>: 分片由一个 主分片 (primary shard)，和可配置数目的多个镜像分片组成。</p>
<ul>
<li>顾名思义，镜像分片 的数据内容和 主分片 保持数据同步。数据写入请求在主分片被完成，并由主分片将写入数据推送(push)到其镜像分片去。</li>
<li>在多节点集群里，同一分片的主分片和镜像分片被分散到了不同的节点。同一个分片里文档的查询请求，既可以在主分片所在节点完成，也可以在镜像分片节点完成。</li>
<li>此外，当主分片所在节点失效时，会将其中一个镜像分片提升为主分片。</li>
<li>因此，更高的分片镜像数目，提供了更好的查询效率，以及更好的容错机制。</li>
</ul>
</li>
</ul>
<p><img src="https://cloud.githubusercontent.com/assets/839287/13384489/f2344cdc-ded0-11e5-8fe0-93c8f634464e.png" alt="cluster"></p>
<p>如图，三个节点构成的集群。有两个索引(A，B)。每个索引设置为3个分片，每个分片镜像数为1:</p>
<ul>
<li>由于镜像分片会在节点间充分平衡，因此，当任意一个节点失效时，都不会导致数据丢失</li>
<li>由于索引个分片均匀分布到三个节点，因此，写入时，总体上可以实现三倍每个节点的写入吞吐</li>
<li>对于每个索引的查询，都可以利用到每台节点的计算能力</li>
</ul>
<h2 id="lucene-">Lucene 是怎么工作的?</h2>
<p>前面我们提到，Elasticsearch 是构建在 Lucene 之上的。那么，Lucene (也就是每个分片) 是如何实现数据的实时写入和查询的呢?</p>
<p>一个 Lucene 由多个 segment 构成。每个 segment 自构建索引信息。在查询的时候，先由分发到每个 segment 执行，然后将结果汇总。数据写入时，是按照 segment 来组织的。为了提高写入速度，数据先写入内存，然后在定期刷回磁盘(commit)。</p>
<p>每个 segment 数据是不可变的，因此删除的时候仅仅标记为删除，相应的数据并没有彻底清除。在写入的过程中，会定期将 segment 的数据合并，在这个合并的过程中删除的数据才真正被清理掉。这也是为什么频繁更新会对性能不好的原因。最优的情况下，每个 Lucene 只有一个 segment，从而所有查询都不需要经过再次聚合。我们也可以手动触发 segment 合并，从而提高单个 Lucene 的查询性能。</p>
<p>写入以及合并的示意图:</p>
<p><img src="https://cloud.githubusercontent.com/assets/839287/13384491/f239c6b2-ded0-11e5-8f12-f51a46efb0ea.png" alt="seg1">
<img src="https://cloud.githubusercontent.com/assets/839287/13384492/f253488a-ded0-11e5-8865-a2be651a9ac0.png" alt="seg2"></p>
<p>如果要做类比的话，我们可以将写入理解为数据库系统 Write-Ahead-Logging (WAL) 的过程。只不过，这里写入的数据都是不变的，因此可以在写入的时候被非常高效地索引，并直接基于这些&quot;日志&quot;(即segments)查询。</p>
<h2 id="-">索引维度的切割</h2>
<p>由于每个索引是固定分片数的，为了优化查询，每个分片 (记得是 Lucene 实例) 索引的数据不能够无限增长。很多时候，我们的写入的是按天分的日志数据。一般的做法是，按天索引。在时间维度上利用索引进行分区，Logstash 默认为 <code>logstash-2016-02-01，logstash-2016-02-02，...</code> 这样按天分表。在查询的时候，可以指定多个索引查询，如<code>logstash-*</code>索引匹配。在有时间范围限定查询的时候，可以提前对于潜在索引过滤，从而减少执行查询所涉及的索引数目。这也是常见的数据库分区查询优化的一个手段。</p>
<h2 id="-">参考</h2>
<ul>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/guide/master/">Elasticsearch: The Definitive Guide</a> 官方文档，啃透了 Elasticsearch 也就熟悉了</li>
<li><a href="http://www.slideshare.net/gugod/elasticsearch-19877436">Elasticsearch 實戰介紹</a> 以及其后宝贵的引用</li>
</ul>
]]></description><link>http://tech.youmi.net/2016/02/137134732.html</link><guid isPermaLink="true">http://tech.youmi.net/2016/02/137134732.html</guid><dc:creator><![CDATA[csyangchen]]></dc:creator><pubDate>Mon, 28 Mar 2016 01:59:21 GMT</pubDate></item><item><title><![CDATA[No title]]></title><description><![CDATA[<p>稍微“古老”一点的互联网时代，我们一直用短信，邮件来进行消息通知，特别是服务器报警这些信息。短信虽好，但是太贵，而且没办法分类分组分级别，后来我们发现了Pushover，很好地解决了我们的问题。</p>
<p>本来我们是一直用Pushover的，但是由于众所周知的原因，Pushover的Android版本在国内没法用，而微信又满足不了我们的需求（分组，分级别推送消息），因此我们自己开发了一个Pushover的替代版：</p>
<p><a href="https://www.alertover.com">https://www.alertover.com</a></p>
<p>免费实用。
我们自己用在服务器报警，运营后台消息推送，已经用了近一年，大家可以放心使用。
同时，我们即将开源Alertover的各类客户端源码。</p>
<p>使用过程中有遇到任何问题，可以发邮件给我们 alertover@youmi.net，或者加我们的官方用户QQ群：313270450</p>
<p><img src="https://cloud.githubusercontent.com/assets/1064207/12672140/1588e560-c6af-11e5-9aa8-f43509ffae9e.png" alt="www alertover com"></p>
]]></description><link>http://tech.youmi.net/2016/01/129715411.html</link><guid isPermaLink="true">http://tech.youmi.net/2016/01/129715411.html</guid><dc:creator><![CDATA[ruitao]]></dc:creator><pubDate>Mon, 28 Mar 2016 01:59:03 GMT</pubDate></item><item><title><![CDATA[No title]]></title><description><![CDATA[<p>Upstart是一个用于替代传统 init 的系统初始化程序。相对于 init 的同步执行，Upstart 是事件驱动、异步工作的。由于是事件驱动， Upstart 提供了传统 init 没法提供的功能，如机器运行时添加或删除U盘；由于异步工作，Upstart 更能充分利用CPU资源，性能更好。</p>
<p>Ubuntu 是我们常用的开发和服务器系统，当前最新 LTS 使用的是 Upstart，了解 Upstart 可以帮我们解答以下这些疑惑：</p>
<ul>
<li>机器启动后的一些初始化脚本应该放在哪里比较合适？</li>
<li>想要在某个通用服务启动前执行一段代码可以怎样做？</li>
<li>设置好开机启动的程序运行时网络是否已经配置好？</li>
<li>硬盘什么时候挂载的？</li>
<li>在一些重启后某些硬盘内容会消失的云主机里什么时候初始化 flashcache？</li>
</ul>
<p>下面介绍一下日常最需要了解 Upstart 的一些方面：</p>
<h3 id="-">一、基本概念</h3>
<p>Upstart 的可执行文件就是系统里的 /sbin/init。它事件驱动的特性体现在通过一些事件来触发任务的执行，比如开机后有 startup 事件、启动一个 job 前有 starting xxx（job 的名字）事件、插入一个U盘也会有相应的事件。Upstart 的任务配置一般写在 /etc/init 目录下（其实还有用户的 Upstart 配置，可以放在 $HOME/.init 目录下）。</p>
<p>Upstart job 的三个类型：</p>
<ol>
<li>Task Job： 运行一小会儿就停止的服务。</li>
<li>Service Job： 一直运行的服务，比如 PHP-FPM、sshd。</li>
<li>Abstract Job：没有 exec 和 script 的任务， Upstart 不会跟踪他的pid， 常用来设置网络服务。</li>
</ol>
<h3 id="-upstart-">二、Upstart 在机器启动时的执行过程</h3>
<p><img src="https://cloud.githubusercontent.com/assets/2314596/12577547/e2abbdee-c455-11e5-99e4-f825db9f3f6b.png" alt="ubuntu_upstart_procedure"></p>
<p>在机器做完那些<a href="http://www.yunweipai.com/archives/782.html">加载内核，挂载跟目录等工作</a>后，操作系统会调用 /sbin/init 来接管后续的服务启动过程。</p>
<ul>
<li>init 启动后马上发出第一个事件 startup</li>
<li><p>hostname，mountall 会被 startup 事件触发（在 /etc/init 目录下 grep startup * 就可以看到了！)，也就是一启动就开始分别设置 hostname 和 挂载硬盘。</p>
<p>  在 mountall.conf 里可以看到它主要运行了 <a href="http://manpages.ubuntu.com/manpages/vivid/man8/mountall.8.html">mountall</a> 程序，这个程序会读取 /lib/init/fstab 和 /etc/fstab 里的配置，按顺序挂载。其中 /lib/init/fstab 里的都是像 /proc /sys 这样的虚拟磁盘。</p>
<p>  mountall 会发出很多可以触发 Upstart 的事件：</p>
<pre><code>  emits virtual<span class="hljs-attr">-filesystems</span>
  emits <span class="hljs-built_in">local</span><span class="hljs-attr">-filesystems</span>
  emits remote<span class="hljs-attr">-filesystems</span>
  emits <span class="hljs-literal">all</span><span class="hljs-attr">-swaps</span>
  emits filesystem
  emits mounting
  emits mounted
</code></pre><p>  每挂载好一个磁盘都会发出一个 mounted 事件，当挂载完 /lib/init/fstab 里的虚拟磁盘后会发出 virtual-filesystems 事件，mountall 继续处理 /etc/fstab 里的配置，期间会发出 remote-filesystems, all-swaps等事件，全部处理完后会发送 local-filesystems 和 filesystem 事件。</p>
<p>  也就是说，当 Upstart 接收到 filesystem 事件时，配置文件中的磁盘已经挂载好了。</p>
<p>  mountall 发出的很多事件，有些是以<a href="http://upstart.ubuntu.com/cookbook/#event-types">阻塞的方式</a>发出的，也就是在 mountall 过程中有些别的任务已经被触发并且可能已经完成了。</p>
</li>
<li><p>udev.conf 在接收到 virtual-filesystem 时出发，这是一个管理 /dev 下的设备描述的程序。</p>
<ul>
<li>udev 导致 upstart-udev-bridge 任务执行，这个任务会设置好 127.0.0.0 这个网络回路地址。</li>
<li>udev 和 filesystem 事件还是开始网络设置的前提。</li>
</ul>
</li>
<li>在设置网络之前，ufw 的配置也会准备就绪。</li>
<li>网络和文件系统都准备好后出发 rc-sysinit.conf 里的脚本执行，里面其实就是打开 telinit 程序并传入一个2作为参数，这个程序发出 runlevel 事件。<ul>
<li>rc.conf 会被 runlevel 事件触发，它会调用 /etc/init.d/rc 2 ， 这是兼容传统 init 的关键步骤。</li>
<li>其他 job 也会被 runlevel 事件触发，例如 cron、ssh、irqbalance 等。</li>
</ul>
</li>
</ul>
<h3 id="-init-">三、 和传统 init 的兼容</h3>
<p>Upstart 兼容传统的 init 也称为 sysV 的启动方式，从上面步骤可以看到在 /etc/init/rc.conf 里调用了 /etc/init.d/rc 2， 这个程序会遍历 /etc/rc2.d 目录下的脚本并执行。</p>
<p>平时我们启动服务会用到下面这几种命令：</p>
<pre><code>service <span class="hljs-keyword">x</span> start
start <span class="hljs-keyword">x</span>
initctl start <span class="hljs-keyword">x</span>
</code></pre><p>其中 service 是一个脚本，它的工作原理是先检查 /etc/init 里的配置有没有这个命令要启动的服务，如果没有才到 /etc/init.d 里找到启动那个服务的脚本并执行。如果你通过 apt-get 安装 PHP-FPM 可以发现 /etc/init.d 里的 PHP-FPM 启动脚本并不会被 service php5-fpm start 调用。</p>
<h3 id="-">参考资料</h3>
<ul>
<li><a href="http://upstart.ubuntu.com/cookbook">Upstart cookbook</a></li>
<li><a href="http://www.yunweipai.com/archives/782.html">Linux操作系统启动过程详解</a></li>
<li><a href="http://manpages.ubuntu.com/manpages/vivid/man8/mountall.8.html">mountall</a></li>
<li><a href="https://zh.wikipedia.org/wiki/Udev">Udev</a></li>
</ul>
]]></description><link>http://tech.youmi.net/2016/01/128743607.html</link><guid isPermaLink="true">http://tech.youmi.net/2016/01/128743607.html</guid><dc:creator><![CDATA[divisoryang]]></dc:creator><pubDate>Mon, 28 Mar 2016 01:58:50 GMT</pubDate></item></channel></rss>